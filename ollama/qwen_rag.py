from langchain_ollama import OllamaLLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import DirectoryLoader, TextLoader

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# 1. Init multilingual embedding model
embedding_model = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={"device": "cpu"}
)

# 2. Your documents
print('generate rag')
loader = DirectoryLoader(
    "docs",  # or "./docs_backup"
    glob="**/*.md",
    loader_cls=lambda path: TextLoader(path, encoding="utf-8"),
    show_progress=True
)
docs = loader.load()

# 3. Split documents
splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
split_docs = splitter.split_documents(docs)

# 4. Create FAISS vector store
vector_db = FAISS.from_documents(split_docs, embedding_model)
retriever = vector_db.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}
)
print('generate rag done')

# 5. Connect to local Ollama model
print('connect llm')
llm = OllamaLLM(
    model="llama3",
    base_url="http://localhost:11435"
)
# llm = OllamaLLM(model="gpt-oss")

# 6. MCP-style Prompt Template
mcp_template = """
ä½ æ˜¯ä¸€å€‹å®¢æœåŠ©ç†ï¼Œæ ¹æ“šä»¥ä¸‹çŸ¥è­˜åº«å…§å®¹å›ç­”ä½¿ç”¨è€…å•é¡Œã€‚
å¦‚æœçŸ¥è­˜åº«ä¸­æ²’æœ‰è¶³å¤ è³‡è¨Šï¼Œè«‹èªªã€Œæ ¹æ“šç›®å‰è³‡è¨Šç„¡æ³•å›ç­”ã€ã€‚
è‹¥æˆ‘æ˜¯ç”¨ç¹é«”ä¸­æ–‡è¼¸å…¥å•é¡Œï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›è¦†ã€‚

ã€çŸ¥è­˜åº«ã€‘
{context}

ã€å•é¡Œã€‘
{question}

ã€å›ç­”ã€‘
""".strip()

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=mcp_template,
)

# 7. Build RAG pipeline with MCP-style prompt
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt},
    return_source_documents=True
)
print('Connect LLM finished')

# 8. Ask questions
while True:
    query = input("ğŸ” å•é¡Œ / Question: ")
    if query.lower() in {"exit", "quit"}:
        break
    result = qa.invoke({"query": query})
    print("\nğŸ“£ å›ç­” / Answer:\n", result['result'])
    print("\nğŸ“š ä¾†æº / Sources:\n", [doc.page_content for doc in result['source_documents']])
    print("\n------------------------------------------------------------------------------------\n")
