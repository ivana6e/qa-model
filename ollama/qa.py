from langchain_ollama import OllamaLLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import DirectoryLoader, TextLoader

from sentence_transformers import SentenceTransformer, util
import warnings

warnings.filterwarnings("ignore", category=FutureWarning)

# 1. Init embedding model
embedding_model = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={"device": "cpu"}
)

# 2. Load documents
loader = DirectoryLoader(
    "docs",
    glob="**/*.md",
    loader_cls=lambda path: TextLoader(path, encoding="utf-8"),
    show_progress=True
)
docs = loader.load()

# 3. Split docs_backup
splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
split_docs = splitter.split_documents(docs)

# 4. Create vector DB
vector_db = FAISS.from_documents(split_docs, embedding_model)
retriever = vector_db.as_retriever()

# 5. Load Ollama LLM
llm = OllamaLLM(model="llama3")

# 6. MCP Prompt template
mcp_template = """
ä½ æ˜¯ä¸€å€‹å®¢æœåŠ©ç†ï¼Œæ ¹æ“šä»¥ä¸‹çŸ¥è­˜åº«å…§å®¹å›ç­”ä½¿ç”¨è€…å•é¡Œã€‚
å¦‚æœçŸ¥è­˜åº«ä¸­æ²’æœ‰è¶³å¤ è³‡è¨Šï¼Œè«‹èªªã€Œæ ¹æ“šç›®å‰è³‡è¨Šç„¡æ³•å›ç­”ã€ã€‚
è‹¥æˆ‘æ˜¯ç”¨ç¹é«”ä¸­æ–‡è¼¸å…¥å•é¡Œï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›è¦†ã€‚

ã€çŸ¥è­˜åº«ã€‘
{context}

ã€å•é¡Œã€‘
{question}

ã€å›ç­”ã€‘
""".strip()

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=mcp_template,
)

# 7. Build QA chain
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt},
    return_source_documents=True
)

# 8. å¤šèªè¨€é—œéµå­—åˆ—è¡¨
unsatisfied_keywords = [
    "ä¸å°", "ä¸æ»¿æ„", "æ²’ç”¨", "æ‰¾çœŸäºº", "å®¢æœ", "ä¸æœƒ", "éŒ¯èª¤",
    "wrong", "unsatisfied", "useless", "human", "agent", "support", "error"
]

# 9. è¼‰å…¥SBERTæ¨¡å‹ï¼ˆæœƒèŠ±é»æ™‚é–“ï¼‰
sbert_model = SentenceTransformer('all-MiniLM-L6-v2')
unsatisfied_examples = [
    "ä¸æ»¿æ„", "æ²’ç”¨", "æ‰¾çœŸäºº", "é€™ç­”æ¡ˆéŒ¯äº†",
    "this answer is wrong", "I want a human agent",
    "not helpful", "unsatisfied"
]
example_embeddings = sbert_model.encode(unsatisfied_examples, convert_to_tensor=True)

def is_unsatisfied(query, keyword_list, sbert_model, example_embeds, threshold=0.7):
    # 1. é—œéµå­—å¿«é€Ÿåˆ¤æ–·
    query_lower = query.lower()
    if any(word in query_lower for word in keyword_list):
        return True
    # 2. SBERTèªæ„ç›¸ä¼¼åº¦åˆ¤æ–·
    query_emb = sbert_model.encode(query, convert_to_tensor=True)
    cos_scores = util.cos_sim(query_emb, example_embeds)
    max_score = cos_scores.max().item()
    return max_score >= threshold

# 10. å•ç­”äº’å‹•èˆ‡åˆ¤æ–·
unsatisfied_count = 0

print("æ™ºèƒ½å®¢æœç³»çµ±å•Ÿå‹•ï¼Œè¼¸å…¥ exit é›¢é–‹ã€‚")
while True:
    query = input("ğŸ” å•é¡Œ / Question: ")
    if query.lower() in {"exit", "quit"}:
        print("æ„Ÿè¬ä½¿ç”¨ï¼Œå†è¦‹ï¼")
        break

    # åˆ¤æ–·ç”¨æˆ¶æ˜¯å¦è¡¨é”ä¸æ»¿æ„
    if is_unsatisfied(query, unsatisfied_keywords, sbert_model, example_embeddings):
        print("ç³»çµ±ï¼šæ‚¨ä¼¼ä¹éœ€è¦çœŸäººå®¢æœï¼Œæ˜¯å¦è¦è½‰æ¥ï¼Ÿ(y/n)")
        choice = input().strip().lower()
        if choice == "y":
            print("ç³»çµ±ï¼šæ­£åœ¨ç‚ºæ‚¨è½‰æ¥çœŸäººå®¢æœï¼Œè«‹ç¨å¾Œ...")
            break
        else:
            print("ç³»çµ±ï¼šç¹¼çºŒç‚ºæ‚¨æä¾›AIå®¢æœæœå‹™ã€‚")

    # AI å›ç­”
    result = qa.invoke({"query": query})
    print("\nğŸ“£ å›ç­” / Answer:\n", result['result'])
    print("\nğŸ“š ä¾†æº / Sources:\n", [doc.page_content for doc in result['source_documents']])
    print("\n------------------------------------------------------------------------------------\n")

    # ç°¡å–®ä¸æ»¿æ„åˆ¤æ–· (ä¾‹å¦‚å›æ‡‰ä¸­å«ã€Œæ ¹æ“šç›®å‰è³‡è¨Šç„¡æ³•å›ç­”ã€)
    if "æ ¹æ“šç›®å‰è³‡è¨Šç„¡æ³•å›ç­”" in result['result']:
        unsatisfied_count += 1
    else:
        unsatisfied_count = 0

    # é€£çºŒå…©æ¬¡å›ç­”ç„¡æ³•è§£æ±ºï¼Œè‡ªå‹•è©¢å•æ˜¯å¦è½‰çœŸäºº
    if unsatisfied_count >= 2:
        print("ç³»çµ±ï¼šçœ‹èµ·ä¾†æˆ‘ç„¡æ³•å›ç­”æ‚¨çš„å•é¡Œï¼Œæ˜¯å¦éœ€è¦è½‰æ¥çœŸäººå®¢æœï¼Ÿ(y/n)")
        choice = input().strip().lower()
        if choice == "y":
            print("ç³»çµ±ï¼šæ­£åœ¨ç‚ºæ‚¨è½‰æ¥çœŸäººå®¢æœï¼Œè«‹ç¨å¾Œ...")
            break
        else:
            unsatisfied_count = 0
            print("ç³»çµ±ï¼šå¥½çš„ï¼Œè«‹ç¹¼çºŒæå•ã€‚")
